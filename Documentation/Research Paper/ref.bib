
@article{promraksa_lane-filtering_2022,
	title = {Lane-{Filtering} {Behavior} of {Motorcycle} {Riders} at {Signalized} {Urban} {Intersections}},
	volume = {2022},
	issn = {0197-6729},
	url = {https://www.hindawi.com/journals/jat/2022/5662117/},
	doi = {10.1155/2022/5662117},
	abstract = {In developing countries, motorcycle riders typically perform lane filtering at signalized urban intersections. This study aims to determine the factors that affect the lateral clearance of motorcycle riders as they travel between two lanes of mixed traffic at signalized urban intersections in developing countries. In this study, an onboard measurement device was developed to measure the lane-filtering behavior of motorcycle riders. It was installed on a test motorcycle to continuously record the lateral clearance, riding behavior, and surrounding traffic conditions. Thirty participants rode the test motorcycle through a signalized urban intersection. Multilevel linear regression was applied to analyze the relationship between lateral clearance and relevant variables at a significance level of 0.05. The instant speed and side of the filtering motorcycle, condition of the lateral vehicle, type of lateral vehicle, and riding frequency of the motorcycle rider significantly influenced the lateral clearance. The findings of this study can contribute to filtering lane management, connected autonomous vehicles, and microscopic traffic simulations for motorcycles traveling in mixed traffic at signalized urban intersections.},
	language = {en},
	urldate = {2023-04-11},
	journal = {Journal of Advanced Transportation},
	author = {Promraksa, Thanapol and Satiennam, Thaned and Satiennam, Wichuda and Kronprasert, Nopadon},
	month = aug,
	year = {2022},
	note = {Publisher: Hindawi},
	pages = {e5662117},
	file = {Full Text PDF:/home/edwardpatch1/Zotero/storage/4C2LABL2/Promraksa et al. - 2022 - Lane-Filtering Behavior of Motorcycle Riders at Si.pdf:application/pdf},
}

@inproceedings{yoshioka_real-time_2017,
	title = {Real-time object classification for autonomous vehicle using {LIDAR}},
	doi = {10.1109/ICIIBMS.2017.8279696},
	abstract = {Object classification is an important issue in order to bring autonomous vehicle into reality. In this paper, real-time and robust classification based on Real AdaBoost algorithm is researched and improved. Various effective features of road objects are computed using LIDAR 3D point clouds. The improved classifier provides an accuracy of over 90 (\%) in a range of 50 (m) and classifies objects into car, pedestrian, bicyclist and background. Moreover, processing time of classifying an object consumes only 0.07×10-3 (sec) that enables this method to be used for autonomous driving on urban roads.},
	booktitle = {2017 {International} {Conference} on {Intelligent} {Informatics} and {Biomedical} {Sciences} ({ICIIBMS})},
	author = {Yoshioka, Masaru and Suganuma, Naoki and Yoneda, Keisuke and Aldibaja, Mohammad},
	month = nov,
	year = {2017},
	note = {ISSN: 2189-8723},
	keywords = {Feature extraction, Three-dimensional displays, Classification algorithms, Roads, Autonomous vehicles, Laser radar, Automobiles, Autonomous Vehicle, LIDAR, Object Classification, Point Cloud, Real AdaBoost},
	pages = {210--211},
	file = {IEEE Xplore Abstract Record:/home/edwardpatch1/Zotero/storage/QGWJB2H7/8279696.html:text/html},
}

@misc{noauthor_tesla_nodate,
	title = {Tesla {Vision} {Update}: {Replacing} {Ultrasonic} {Sensors} with {Tesla} {Vision} {\textbar} {Tesla} {Support} {United} {Kingdom}},
	shorttitle = {Tesla {Vision} {Update}},
	url = {https://www.tesla.com/en_gb/support/transitioning-tesla-vision},
	abstract = {Safety is at the core of our design and engineering decisions. In 2021, we began our transition to Tesla Vision by removing radar from Model 3 and Model Y, followed by Model S and Model X in 2022. Today, in most regions around the globe, these vehicles now rely on Tesla Vision, our camera-based Autopilot system.},
	language = {en-GB},
	urldate = {2023-07-21},
	journal = {Tesla},
	file = {Snapshot:/home/edwardpatch1/Zotero/storage/SLCX23A3/transitioning-tesla-vision.html:text/html},
}

@misc{eduonix_real-world_2022,
	title = {Real-{World} {Implementations} {Of} {YOLO} {Algorithm}},
	url = {https://blog.eduonix.com/software-development/real-world-implementations-of-yolo-algorithm/},
	abstract = {YOLO stands for ‘You Only Look Once’ and this algorithm is an excellent object-detection algorithm that uses convolutional neural networks.},
	language = {en-US},
	urldate = {2023-07-21},
	journal = {Eduonix Blog},
	author = {Eduonix, Tutor @},
	month = jan,
	year = {2022},
	file = {Snapshot:/home/edwardpatch1/Zotero/storage/UKZCJUTI/real-world-implementations-of-yolo-algorithm.html:text/html},
}

@misc{noauthor_computer_2020,
	title = {Computer {Vision} at {Tesla} for {Self}-{Driving} {Cars}},
	url = {https://www.thinkautonomous.ai/blog/computer-vision-at-tesla/},
	abstract = {Recently, Tesla has released "Tesla Vision", their new system equipped only with cameras... making it one of the only companies in the world not to use RADARs!},
	language = {en},
	urldate = {2023-07-21},
	journal = {Welcome to The Library!},
	month = jul,
	year = {2020},
	file = {Snapshot:/home/edwardpatch1/Zotero/storage/MK63QKGT/computer-vision-at-tesla.html:text/html},
}

@misc{govuk_self-driving_2022,
	type = {Government {Article}},
	title = {Self-driving revolution to boost economy and improve road safety},
	url = {https://www.gov.uk/government/news/self-driving-revolution-to-boost-economy-and-improve-road-safety},
	abstract = {New plan for self-driving vehicles plus a consultation on a safety ambition.},
	language = {en},
	urldate = {2023-07-21},
	journal = {Self-driving revolution to boost economy and improve road safety},
	author = {{GOV.UK}},
	month = aug,
	year = {2022},
	file = {Snapshot:/home/edwardpatch1/Zotero/storage/F3TXLD2Y/self-driving-revolution-to-boost-economy-and-improve-road-safety.html:text/html},
}

@article{li_what_2021,
	title = {What {Happens} for a {ToF} {LiDAR} in {Fog}?},
	volume = {22},
	issn = {1558-0016},
	doi = {10.1109/TITS.2020.2998077},
	abstract = {By transmitting lasers and processing laser returns, LiDAR (light detection and ranging) perceives the surrounding environment through distance measurements. Because of high ranging accuracy, LiDAR is one of the most critical sensors in autonomous driving systems. Revolving around the 3D point clouds generated from LiDARs, plentiful algorithms have been developed for object detection/tracking, environmental mapping, or localization. However, a LiDAR’s ranging performance suffers under adverse weather (e.g. fog, rain, snow etc.), which impedes full autonomous driving in all weather conditions. This article focuses on analyzing the performance of a typical time-of-flight (ToF) LiDAR under fog environment. By controlling the fog density within CEREMA Adverse Weather Facility, the relations between the ranging performance and fogs are both qualitatively and quantitatively investigated. Furthermore, based on the collected data, a machine learning based model is trained to predict the minimum fog visibility that allows successful ranging for this type of LiDAR. The revealed experimental results and methods are helpful for ToF LiDAR specifications from automotive industry.},
	number = {11},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Li, You and Duthon, Pierre and Colomb, Michèle and Ibanez-Guzman, Javier},
	month = nov,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {adverse weather, autonomous vehicles, Distance measurement, laser noise, Laser radar, Measurement by laser beam, Rain, Surface emitting lasers},
	pages = {6670--6681},
	file = {IEEE Xplore Abstract Record:/home/edwardpatch1/Zotero/storage/IMHACXCF/9121741.html:text/html},
}
