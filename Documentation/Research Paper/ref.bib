
@article{promraksa_lane-filtering_2022,
	title = {Lane-{Filtering} {Behavior} of {Motorcycle} {Riders} at {Signalized} {Urban} {Intersections}},
	volume = {2022},
	issn = {0197-6729},
	url = {https://www.hindawi.com/journals/jat/2022/5662117/},
	doi = {10.1155/2022/5662117},
	abstract = {In developing countries, motorcycle riders typically perform lane filtering at signalized urban intersections. This study aims to determine the factors that affect the lateral clearance of motorcycle riders as they travel between two lanes of mixed traffic at signalized urban intersections in developing countries. In this study, an onboard measurement device was developed to measure the lane-filtering behavior of motorcycle riders. It was installed on a test motorcycle to continuously record the lateral clearance, riding behavior, and surrounding traffic conditions. Thirty participants rode the test motorcycle through a signalized urban intersection. Multilevel linear regression was applied to analyze the relationship between lateral clearance and relevant variables at a significance level of 0.05. The instant speed and side of the filtering motorcycle, condition of the lateral vehicle, type of lateral vehicle, and riding frequency of the motorcycle rider significantly influenced the lateral clearance. The findings of this study can contribute to filtering lane management, connected autonomous vehicles, and microscopic traffic simulations for motorcycles traveling in mixed traffic at signalized urban intersections.},
	language = {en},
	urldate = {2023-04-11},
	journal = {Journal of Advanced Transportation},
	author = {Promraksa, Thanapol and Satiennam, Thaned and Satiennam, Wichuda and Kronprasert, Nopadon},
	month = aug,
	year = {2022},
	note = {Publisher: Hindawi},
	pages = {e5662117},
	file = {Full Text PDF:/home/skira21/Zotero/storage/4C2LABL2/Promraksa et al. - 2022 - Lane-Filtering Behavior of Motorcycle Riders at Si.pdf:application/pdf},
}

@misc{govuk_self-driving_2022,
	type = {Government {Article}},
	title = {Self-driving revolution to boost economy and improve road safety},
	url = {https://www.gov.uk/government/news/self-driving-revolution-to-boost-economy-and-improve-road-safety},
	abstract = {New plan for self-driving vehicles plus a consultation on a safety ambition.},
	language = {en},
	urldate = {2023-07-21},
	journal = {Self-driving revolution to boost economy and improve road safety},
	author = {{GOV.UK}},
	month = aug,
	year = {2022},
	file = {Snapshot:/home/skira21/Zotero/storage/F3TXLD2Y/self-driving-revolution-to-boost-economy-and-improve-road-safety.html:text/html},
}

@misc{noauthor_computer_2020,
	title = {Computer {Vision} at {Tesla} for {Self}-{Driving} {Cars}},
	url = {https://www.thinkautonomous.ai/blog/computer-vision-at-tesla/},
	abstract = {Recently, Tesla has released "Tesla Vision", their new system equipped only with cameras... making it one of the only companies in the world not to use RADARs!},
	language = {en},
	urldate = {2023-07-21},
	journal = {Welcome to The Library!},
	month = jul,
	year = {2020},
	file = {Snapshot:/home/skira21/Zotero/storage/MK63QKGT/computer-vision-at-tesla.html:text/html},
}

@misc{eduonix_real-world_2022,
	title = {Real-{World} {Implementations} {Of} {YOLO} {Algorithm}},
	url = {https://blog.eduonix.com/software-development/real-world-implementations-of-yolo-algorithm/},
	abstract = {YOLO stands for ‘You Only Look Once’ and this algorithm is an excellent object-detection algorithm that uses convolutional neural networks.},
	language = {en-US},
	urldate = {2023-07-21},
	journal = {Eduonix Blog},
	author = {Eduonix, Tutor @},
	month = jan,
	year = {2022},
	file = {Snapshot:/home/skira21/Zotero/storage/UKZCJUTI/real-world-implementations-of-yolo-algorithm.html:text/html},
}

@misc{noauthor_tesla_nodate,
	title = {Tesla {Vision} {Update}: {Replacing} {Ultrasonic} {Sensors} with {Tesla} {Vision} {\textbar} {Tesla} {Support} {United} {Kingdom}},
	shorttitle = {Tesla {Vision} {Update}},
	url = {https://www.tesla.com/en_gb/support/transitioning-tesla-vision},
	abstract = {Safety is at the core of our design and engineering decisions. In 2021, we began our transition to Tesla Vision by removing radar from Model 3 and Model Y, followed by Model S and Model X in 2022. Today, in most regions around the globe, these vehicles now rely on Tesla Vision, our camera-based Autopilot system.},
	language = {en-GB},
	urldate = {2023-07-21},
	journal = {Tesla},
	file = {Snapshot:/home/skira21/Zotero/storage/SLCX23A3/transitioning-tesla-vision.html:text/html},
}

@inproceedings{yoshioka_real-time_2017,
	title = {Real-time object classification for autonomous vehicle using {LIDAR}},
	doi = {10.1109/ICIIBMS.2017.8279696},
	abstract = {Object classification is an important issue in order to bring autonomous vehicle into reality. In this paper, real-time and robust classification based on Real AdaBoost algorithm is researched and improved. Various effective features of road objects are computed using LIDAR 3D point clouds. The improved classifier provides an accuracy of over 90 (\%) in a range of 50 (m) and classifies objects into car, pedestrian, bicyclist and background. Moreover, processing time of classifying an object consumes only 0.07×10-3 (sec) that enables this method to be used for autonomous driving on urban roads.},
	booktitle = {2017 {International} {Conference} on {Intelligent} {Informatics} and {Biomedical} {Sciences} ({ICIIBMS})},
	author = {Yoshioka, Masaru and Suganuma, Naoki and Yoneda, Keisuke and Aldibaja, Mohammad},
	month = nov,
	year = {2017},
	note = {ISSN: 2189-8723},
	keywords = {Feature extraction, Three-dimensional displays, Classification algorithms, Autonomous vehicles, Roads, Automobiles, Autonomous Vehicle, Laser radar, LIDAR, Object Classification, Point Cloud, Real AdaBoost},
	pages = {210--211},
	file = {IEEE Xplore Abstract Record:/home/skira21/Zotero/storage/QGWJB2H7/8279696.html:text/html},
}

@article{royo_overview_2019,
	title = {An {Overview} of {Lidar} {Imaging} {Systems} for {Autonomous} {Vehicles}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/19/4093},
	doi = {10.3390/app9194093},
	abstract = {Lidar imaging systems are one of the hottest topics in the optronics industry. The need to sense the surroundings of every autonomous vehicle has pushed forward a race dedicated to deciding the final solution to be implemented. However, the diversity of state-of-the-art approaches to the solution brings a large uncertainty on the decision of the dominant final solution. Furthermore, the performance data of each approach often arise from different manufacturers and developers, which usually have some interest in the dispute. Within this paper, we intend to overcome the situation by providing an introductory, neutral overview of the technology linked to lidar imaging systems for autonomous vehicles, and its current state of development. We start with the main single-point measurement principles utilized, which then are combined with different imaging strategies, also described in the paper. An overview of the features of the light sources and photodetectors specific to lidar imaging systems most frequently used in practice is also presented. Finally, a brief section on pending issues for lidar development in autonomous vehicles has been included, in order to present some of the problems which still need to be solved before implementation may be considered as final. The reader is provided with a detailed bibliography containing both relevant books and state-of-the-art papers for further progress in the subject.},
	language = {en},
	number = {19},
	urldate = {2023-08-02},
	journal = {Applied Sciences},
	author = {Royo, Santiago and Ballesta-Garcia, Maria},
	month = jan,
	year = {2019},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D imaging, autonomous vehicles, ladar, lasers, lidar, MEMS, photodetectors, point cloud, scanners, self-driving car, time of flight},
	pages = {4093},
	file = {Full Text PDF:/home/skira21/Zotero/storage/ZFINE7P5/Royo and Ballesta-Garcia - 2019 - An Overview of Lidar Imaging Systems for Autonomou.pdf:application/pdf},
}

@article{li_what_2021,
	title = {What {Happens} for a {ToF} {LiDAR} in {Fog}?},
	volume = {22},
	issn = {1558-0016},
	doi = {10.1109/TITS.2020.2998077},
	abstract = {By transmitting lasers and processing laser returns, LiDAR (light detection and ranging) perceives the surrounding environment through distance measurements. Because of high ranging accuracy, LiDAR is one of the most critical sensors in autonomous driving systems. Revolving around the 3D point clouds generated from LiDARs, plentiful algorithms have been developed for object detection/tracking, environmental mapping, or localization. However, a LiDAR’s ranging performance suffers under adverse weather (e.g. fog, rain, snow etc.), which impedes full autonomous driving in all weather conditions. This article focuses on analyzing the performance of a typical time-of-flight (ToF) LiDAR under fog environment. By controlling the fog density within CEREMA Adverse Weather Facility, the relations between the ranging performance and fogs are both qualitatively and quantitatively investigated. Furthermore, based on the collected data, a machine learning based model is trained to predict the minimum fog visibility that allows successful ranging for this type of LiDAR. The revealed experimental results and methods are helpful for ToF LiDAR specifications from automotive industry.},
	number = {11},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Li, You and Duthon, Pierre and Colomb, Michèle and Ibanez-Guzman, Javier},
	month = nov,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Rain, Laser radar, autonomous vehicles, adverse weather, Distance measurement, laser noise, Measurement by laser beam, Surface emitting lasers},
	pages = {6670--6681},
	file = {IEEE Xplore Abstract Record:/home/skira21/Zotero/storage/IMHACXCF/9121741.html:text/html},
}

@article{wang_pedestrian_2017,
	title = {Pedestrian recognition and tracking using {3D} {LiDAR} for autonomous vehicle},
	volume = {88},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921889015302633},
	doi = {10.1016/j.robot.2016.11.014},
	abstract = {This paper studies the pedestrian recognition and tracking problem for autonomous vehicles using a 3D LiDAR, a classifier trained by SVM (Support Vector Machine) is used to recognize pedestrians, the recognition performance is further improved with the aid of tracking results. By comparing positions and velocity directions of pedestrians with curb information, alarms will be generated if pedestrians are detected to be on road or close to curbs. The proposed approach has been verified on an autonomous vehicle platform.},
	language = {en},
	urldate = {2023-08-02},
	journal = {Robotics and Autonomous Systems},
	author = {Wang, Heng and Wang, Bin and Liu, Bingbing and Meng, Xiaoli and Yang, Guanghong},
	month = feb,
	year = {2017},
	pages = {71--78},
	file = {Wang et al. - 2017 - Pedestrian recognition and tracking using 3D LiDAR.pdf:/home/skira21/Zotero/storage/LFGGMPXE/Wang et al. - 2017 - Pedestrian recognition and tracking using 3D LiDAR.pdf:application/pdf},
}
